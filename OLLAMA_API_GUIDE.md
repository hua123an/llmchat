# 🦙 Ollama API 集成指南

## 📋 概述

ChatLLM 现已完全集成 Ollama 的官方 API，支持动态获取模型列表、实时刷新功能，并且完全无需 API 密钥，让本地 AI 体验更加便捷。

## 🎯 核心功能

### ✅ 已实现的功能

**1. 动态模型列表**
- 使用 `/api/tags` 端点实时获取已安装的模型
- 自动识别模型名称、大小、修改时间
- 智能去除 `:latest` 后缀，显示更清晰

**2. 无密钥集成**
- ⚠️ **本地 Ollama 完全无需 API 密钥**
- 移除了对密钥的检查和验证逻辑
- 基于纯 HTTP API，完全本地化

**3. 一键刷新功能**
- 在 ProvidersFloating 悬浮窗添加 🦙 刷新按钮
- 实时检测 Ollama 服务状态
- 智能错误提示和用户引导

**4. 增强错误处理**
- 服务未启动提示：`ollama serve`
- 无模型提示：`ollama pull llama3.2`
- 连接超时和网络错误处理

## 🚀 使用方式

### 1. 配置 Ollama 提供商

在 ChatLLM 中添加提供商：
- **名称**: `Ollama`
- **Base URL**: `http://localhost:11434`
- **API 密钥**: **留空即可** ⚠️

### 2. 动态刷新模型

**方法一：ProvidersFloating 悬浮窗**
1. 点击右下角 ⚙️ 配置按钮
2. 点击 🦙 **刷新Ollama** 按钮
3. 系统自动获取最新模型列表

**方法二：设置页面**
1. 打开设置 → 模型配置
2. 查看并管理 Ollama 提供商
3. 测试连接状态

### 3. 模型选择

刷新后，在聊天界面：
1. 选择 🦙 **Ollama** 提供商
2. 从下拉列表选择已下载的模型
3. 开始本地 AI 对话

## 🔧 技术细节

### API 端点使用

| 端点 | 用途 | 实现状态 |
|------|------|----------|
| `/api/chat` | 流式对话 | ✅ 完整实现 |
| `/api/tags` | 获取模型列表 | ✅ 完整实现 |
| `/api/show` | 模型详情 | 📋 规划中 |
| `/api/pull` | 下载模型 | 📋 规划中 |

### 响应处理

**模型列表响应：**
```json
{
  "models": [
    {
      "name": "llama3.2:latest",
      "size": 2019393792,
      "digest": "sha256:...",
      "modified_at": "2024-01-01T12:00:00Z"
    }
  ]
}
```

**处理逻辑：**
- 提取 `name` 字段作为模型 ID
- 计算 `size` 转换为 GB 显示
- 去除 `:latest` 后缀美化显示
- 记录修改时间供排序使用

### 错误状态检测

```javascript
// 连接失败检测
if (error.message.includes('ECONNREFUSED')) {
  // 提示用户启动 Ollama 服务
  message = 'Ollama服务未运行，请先启动：ollama serve';
}

// 空模型列表处理
if (models.length === 0) {
  // 引导用户下载模型
  console.warn('请先下载模型：ollama pull llama3.2');
}
```

## 📊 性能优化

### 1. 智能缓存
- 模型列表会在前端缓存
- 只有手动刷新才重新请求
- 避免频繁 API 调用

### 2. 超时控制
- API 请求超时设置为 5-8 秒
- 快速失败，避免界面卡顿
- 友好的加载状态提示

### 3. 异步处理
- 所有 API 调用都是异步的
- 不阻塞主界面操作
- 支持并发多个请求

## 🛠️ 故障排除

### 常见问题

**Q: 刷新按钮是灰色的？**
A: 请先添加 Ollama 提供商，确保 Base URL 包含 `localhost:11434`

**Q: 提示"Ollama服务未运行"？**
A: 在终端运行 `ollama serve` 启动服务

**Q: 模型列表为空？**
A: 运行 `ollama pull llama3.2` 下载模型，然后刷新

**Q: 连接超时？**
A: 检查防火墙设置，确保 11434 端口可访问

### 调试信息

在开发者控制台查看详细日志：
```javascript
// 模型获取日志
🦙 正在刷新Ollama模型列表...
🦙 Ollama发现 3 个已安装的模型: ["llama3.2", "qwen2.5:7b", "mistral:7b"]
🦙 Ollama刷新完成，发现 3 个模型

// 错误日志
🦙 Ollama模型列表刷新失败: fetch failed
```

## 🔮 未来规划

### 即将推出的功能

**1. 模型管理**
- `/api/pull` - 在 ChatLLM 内直接下载模型
- `/api/delete` - 删除不需要的模型
- 模型大小和存储使用情况显示

**2. 模型详情**
- `/api/show` - 显示模型参数和配置
- 支持的功能（对话、代码、多模态）标识
- 性能基准和推荐用途

**3. 高级配置**
- 自定义模型文件路径
- GPU 加速配置检测
- 内存使用优化建议

**4. 批量操作**
- 一键更新所有模型
- 批量删除旧版本模型
- 模型备份和迁移

## 💡 最佳实践

### 1. 模型选择建议
- **日常对话**: `llama3.2` (3B，快速响应)
- **编程助手**: `codellama:7b` (代码生成专用)
- **中文优化**: `qwen2.5:7b` (中英双语)
- **高质量**: `llama3.1:70b` (需要大内存)

### 2. 性能优化建议
- 定期清理不用的模型释放空间
- 使用 SSD 存储提升加载速度
- 配置足够的系统内存
- 关闭其他占用 GPU 的应用

### 3. 安全考虑
- Ollama 完全本地运行，数据不离开设备
- 无需担心 API 密钥泄露风险
- 可以完全离线使用
- 支持自定义模型和微调

---

**提示**: 这个集成基于 Ollama 官方 API 文档实现，确保最大兼容性和稳定性。如有问题，请参考 [Ollama API 文档](https://github.com/ollama/ollama/blob/main/docs/api.md)。
