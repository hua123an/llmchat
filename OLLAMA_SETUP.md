# ğŸ¦™ Ollama æœ¬åœ°æ¨¡å‹é…ç½®æŒ‡å—

## ğŸ“‹ ç®€ä»‹

Ollama æ˜¯ä¸€ä¸ªåœ¨æœ¬åœ°è¿è¡Œå¤§è¯­è¨€æ¨¡å‹çš„å·¥å…·ï¼ŒChatLLM ç°å·²æ”¯æŒ Ollama é›†æˆï¼Œè®©æ‚¨å¯ä»¥ä½¿ç”¨å®Œå…¨æœ¬åœ°åŒ–çš„ AI æ¨¡å‹ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£… Ollama

**macOS/Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Windows:**
ä¸‹è½½å®‰è£…ç¨‹åºï¼š[https://ollama.ai/download/windows](https://ollama.ai/download/windows)

### 2. å¯åŠ¨ Ollama æœåŠ¡

```bash
ollama serve
```

Ollama æœåŠ¡å°†åœ¨ `http://localhost:11434` è¿è¡Œã€‚

### 3. ä¸‹è½½æ¨¡å‹

**æ¨èæ¨¡å‹ï¼š**

```bash
# Llama 3.2 (3B) - è½»é‡çº§ï¼Œé€‚åˆæ—¥å¸¸å¯¹è¯
ollama pull llama3.2

# Qwen2.5 7B - ä¸­è‹±æ–‡æ”¯æŒä¼˜ç§€
ollama pull qwen2.5:7b

# Mistral 7B - ä»£ç ç”Ÿæˆèƒ½åŠ›å¼º
ollama pull mistral:7b

# Code Llama 7B - ä¸“é—¨ç”¨äºä»£ç 
ollama pull codellama:7b

# Gemma 2 9B - Google å¼€æºæ¨¡å‹
ollama pull gemma2:9b
```

**æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹ï¼š**
```bash
ollama list
```

## ğŸ”§ åœ¨ ChatLLM ä¸­é…ç½® Ollama

### 1. æ·»åŠ  Provider

åœ¨ ChatLLM è®¾ç½®ä¸­æ·»åŠ æ–°çš„ Providerï¼š

- **Provider åç§°**: `Ollama`
- **API ç«¯ç‚¹**: `http://localhost:11434`
- **API å¯†é’¥**: ç•™ç©ºå³å¯ âš ï¸ ï¼ˆæœ¬åœ° Ollama æ— éœ€å¯†é’¥ï¼‰

### 2. ç¯å¢ƒå˜é‡ (å¯é€‰)

å¦‚æœæ‚¨éœ€è¦è‡ªå®šä¹‰é…ç½®ï¼Œå¯ä»¥åœ¨ `.env` æ–‡ä»¶ä¸­è®¾ç½®ï¼š

```env
# Ollama é…ç½®
OLLAMA_API_URL=http://localhost:11434
OLLAMA_API_KEY=  # æœ¬åœ°Ollamaæ— éœ€å¯†é’¥ï¼Œç•™ç©ºå³å¯
ENABLE_OLLAMA=true
```

## ğŸ¯ æ¨¡å‹æ¨è

### æŒ‰ç”¨é€”åˆ†ç±»

**ğŸ“ æ—¥å¸¸å¯¹è¯**
- `llama3.2:latest` - æœ€æ–° Llama æ¨¡å‹ï¼Œå¹³è¡¡æ€§èƒ½
- `qwen2.5:7b` - ä¸­æ–‡æ”¯æŒä¼˜ç§€

**ğŸ’» ç¼–ç¨‹è¾…åŠ©**
- `codellama:7b` - ä¸“é—¨ç”¨äºä»£ç ç”Ÿæˆ
- `mistral:7b` - ä»£ç ç†è§£èƒ½åŠ›å¼º

**ğŸŒ å¤šè¯­è¨€æ”¯æŒ**
- `qwen2.5:7b` - ä¸­è‹±æ–‡åŒè¯­
- `gemma2:9b` - å¤šè¯­è¨€æ”¯æŒ

**âš¡ æ€§èƒ½è¦æ±‚**
- è½»é‡çº§ï¼š`llama3.2:3b`, `qwen2.5:3b`
- é«˜æ€§èƒ½ï¼š`llama3.1:70b`, `qwen2.5:14b`

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: Ollama æœåŠ¡è¿æ¥å¤±è´¥**
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
ollama ps

# é‡å¯æœåŠ¡
ollama serve
```

**Q: æ¨¡å‹åˆ—è¡¨ä¸ºç©º**
```bash
# ç¡®ä¿å·²å®‰è£…æ¨¡å‹
ollama list

# å¦‚æœæ²¡æœ‰æ¨¡å‹ï¼Œä¸‹è½½ä¸€ä¸ª
ollama pull llama3.2
```

**Q: å“åº”é€Ÿåº¦æ…¢**
- è€ƒè™‘ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ (3B-7B)
- ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç³»ç»Ÿå†…å­˜ (è‡³å°‘ 8GB)
- å…³é—­å…¶ä»–å ç”¨ GPU/CPU çš„åº”ç”¨

### ç³»ç»Ÿè¦æ±‚

**æœ€ä½é…ç½®ï¼š**
- RAM: 8GB (3B æ¨¡å‹)
- å­˜å‚¨: 4GB å¯ç”¨ç©ºé—´
- CPU: ç°ä»£å¤šæ ¸å¤„ç†å™¨

**æ¨èé…ç½®ï¼š**
- RAM: 16GB+ (7B+ æ¨¡å‹)
- GPU: NVIDIA RTX ç³»åˆ— (å¯é€‰ï¼ŒåŠ é€Ÿæ¨ç†)
- å­˜å‚¨: 20GB+ SSD

## ğŸš€ é«˜çº§é…ç½®

### è‡ªå®šä¹‰æ¨¡å‹å‚æ•°

æ ¹æ® [Ollama å®˜æ–¹ API æ–‡æ¡£](https://github.com/ollama/ollama/blob/main/docs/api.md)ï¼ŒChatLLM æ”¯æŒä»¥ä¸‹å®˜æ–¹å‚æ•°ï¼š

```javascript
// åœ¨ electron/main.ts ä¸­çš„ ollamaRequestBody.options
options: {
  temperature: 0.7,        // åˆ›é€ æ€§æ§åˆ¶ (0.0-2.0ï¼Œé»˜è®¤0.8)
  top_k: 40,               // Top-ké‡‡æ · (é»˜è®¤40)
  top_p: 0.9,              // Top-pé‡‡æ · (é»˜è®¤0.9)
  num_predict: 4096,       // æœ€å¤§ç”Ÿæˆtokensæ•° (é»˜è®¤128ï¼Œ-1æ— é™åˆ¶)
  repeat_penalty: 1.1,     // é‡å¤æƒ©ç½š (é»˜è®¤1.1)
  seed: -1,                // éšæœºç§å­ (-1ä¸ºéšæœº)
  stop: [],                // åœæ­¢è¯åˆ—è¡¨
  num_ctx: 2048,           // ä¸Šä¸‹æ–‡çª—å£å¤§å° (é»˜è®¤2048)
  num_thread: 8,           // çº¿ç¨‹æ•° (é»˜è®¤æ£€æµ‹CPUæ ¸å¿ƒæ•°)
  mirostat: 0,             // Mirostaté‡‡æ · (0=ç¦ç”¨, 1=Mirostat, 2=Mirostat 2.0)
  mirostat_eta: 0.1,       // Mirostatå­¦ä¹ ç‡ (é»˜è®¤0.1)
  mirostat_tau: 5.0        // Mirostatç›®æ ‡ç†µ (é»˜è®¤5.0)
}
```

### æ¨¡å‹æ€§èƒ½è°ƒä¼˜å»ºè®®

**å¯¹è¯è´¨é‡ä¼˜å…ˆï¼š**
```javascript
{
  temperature: 0.7,
  top_p: 0.9,
  repeat_penalty: 1.1,
  num_ctx: 4096
}
```

**å¿«é€Ÿå“åº”ä¼˜å…ˆï¼š**
```javascript
{
  temperature: 0.3,
  top_k: 20,
  num_predict: 2048,
  num_ctx: 2048,
  num_thread: 8
}
```

**åˆ›æ„å†™ä½œä¼˜åŒ–ï¼š**
```javascript
{
  temperature: 0.9,
  top_p: 0.95,
  repeat_penalty: 1.05,
  mirostat: 2,
  mirostat_tau: 4.0
}
```

### GPU åŠ é€Ÿ (å¯é€‰)

å¦‚æœæ‚¨æœ‰ NVIDIA GPUï¼ŒOllama ä¼šè‡ªåŠ¨ä½¿ç”¨ CUDA åŠ é€Ÿï¼š

```bash
# éªŒè¯ GPU æ”¯æŒ
nvidia-smi

# æŸ¥çœ‹ Ollama GPU ä½¿ç”¨æƒ…å†µ
ollama ps
```

## ğŸ”§ API ç«¯ç‚¹è¯´æ˜

æ ¹æ® [å®˜æ–¹ API æ–‡æ¡£](https://github.com/ollama/ollama/blob/main/docs/api.md)ï¼ŒOllama æä¾›ä»¥ä¸‹ä¸»è¦ç«¯ç‚¹ï¼š

| ç«¯ç‚¹ | æ–¹æ³• | ç”¨é€” | ChatLLM ä¸­çš„ä½¿ç”¨ |
|------|------|------|------------------|
| `/api/chat` | POST | æµå¼å¯¹è¯ | âœ… ä¸»è¦èŠå¤©æ¥å£ |
| `/api/tags` | GET | è·å–æ¨¡å‹åˆ—è¡¨ | âœ… æ¨¡å‹é€‰æ‹©å™¨ |
| `/api/show` | POST | è·å–æ¨¡å‹è¯¦æƒ… | ğŸ“‹ å¾…å®ç° |
| `/api/pull` | POST | ä¸‹è½½æ¨¡å‹ | ğŸ“‹ å¾…å®ç° |
| `/api/create` | POST | åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹ | ğŸ“‹ å¾…å®ç° |

### å“åº”æ ¼å¼ç¤ºä¾‹

**æµå¼èŠå¤©å“åº”ï¼š**
```json
{
  "message": {
    "role": "assistant",
    "content": "Hello! How can I help you today?"
  },
  "done": false
}
```

**æœ€ç»ˆå“åº”ï¼ˆå«æ€§èƒ½æŒ‡æ ‡ï¼‰ï¼š**
```json
{
  "message": {
    "role": "assistant",
    "content": ""
  },
  "done": true,
  "total_duration": 5589157167,
  "load_duration": 3013701500,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 325953000,
  "eval_count": 290,
  "eval_duration": 2240486000
}
```

## ğŸ“š æ›´å¤šèµ„æº

- [Ollama å®˜æ–¹ API æ–‡æ¡£](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Ollama å®˜æ–¹æ–‡æ¡£](https://ollama.ai/docs)
- [æ¨¡å‹åº“](https://ollama.ai/library)
- [GitHub ä»“åº“](https://github.com/ollama/ollama)

## ğŸ‰ å¼€å§‹ä½¿ç”¨

é…ç½®å®Œæˆåï¼Œåœ¨ ChatLLM çš„æ¨¡å‹é€‰æ‹©å™¨ä¸­ï¼š

1. é€‰æ‹© `Ollama` æä¾›å•†
2. é€‰æ‹©å·²å®‰è£…çš„æ¨¡å‹ (å¦‚ `llama3.2:latest`)
3. å¼€å§‹æœ¬åœ°åŒ– AI å¯¹è¯ï¼

---

**æç¤º**: é¦–æ¬¡ä½¿ç”¨æ—¶ï¼Œå¦‚æœæ¨¡å‹åˆ—è¡¨ä¸ºç©ºï¼Œè¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œå¹¶ä¸”å·²å®‰è£…è‡³å°‘ä¸€ä¸ªæ¨¡å‹ã€‚
